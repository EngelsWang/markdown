# Word Embedding
1 of N Encoding

word class

word embedding

how about auto-encoder?NO

## How to exploit the context?
1. count based
   
   ![](img/countbased.PNG)
2. Prediction based

    ![](img/predictionbased.PNG)

    Prediction-based -- Sharing Parameters

    ![](img/pbsp.PNG)

    Word embedding == W * 1 of N
    1. Training
        ![](img/pdtraining.PNG)
    2. Various Architecures
       1. continuous bag of word model
       2. skip-gram
        ![](img/pbva.PNG)

## Multi-lingual Embedding
## Multi-domain Embedding
zero-shot
## Doument Embdding
## Beyond Bag of Word 